# -*- coding: utf-8 -*-
"""Copy of Copy of PATE GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UnC2Myw5Inx7bFvMbrkCTnKuyhAAxcgP
"""

"""
Credits: The following code is adapted and modified for our experiments from the GitHub repository of the original authors (Jordan et al.) of PATE-GAN research paper (https://openreview.net/forum?id=S1zk9iRqF7).
Link to the original code repository: https://github.com/vanderschaarlab/mlforhealthlabpub/tree/main/alg/pategan
"""

# import packages
import argparse
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow.compat.v1 as tf
from sklearn import metrics
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
import warnings
tf.disable_v2_behavior()
warnings.filterwarnings("ignore")

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

# Various prediction models
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier
from xgboost import XGBRegressor

# load census income dataset
census_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header=None) 

# assign column names
census_df.columns = census_df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 
                     'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']
columns = census_df.columns

# remove final weight feature as it has no use for our task
census_df.drop(['fnlwgt'], axis=1, inplace=True)

# replace and drop rows containing ' ?' values
census_df.replace(' ?', np.NaN, inplace=True)
census_df.dropna(inplace=True)

# encode the categorical columns
enc = preprocessing.LabelEncoder()

class MultiColumnLabelEncoder:
    def __init__(self,columns = None):
        self.columns = columns # array of column names to encode

    def fit(self,X,y=None):
        return self # not relevant here

    def transform(self,X):
        '''
        Transforms columns of X specified in self.columns using
        LabelEncoder(). If no columns specified, transforms all
        columns in X.
        '''
        output = X.copy()
        if self.columns is not None:
            for col in self.columns:
                output[col] = LabelEncoder().fit_transform(output[col])
        else:
            for colname,col in output.iteritems():
                output[colname] = LabelEncoder().fit_transform(col)
        return output

    def fit_transform(self,X,y=None):
        return self.fit(X,y).transform(X)

# encode the following columns
census_df_enc = MultiColumnLabelEncoder(columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']).fit_transform(census_df)

def data_generator(no, dim, noise_rate):
  
  """Generate train/test dataset for PATE-GAN evaluation
  
  Args:
    no: The number of train/test samples
    dim: The number of dimensions in train/test features
    noise_rate: The amount of noise for generating labels
    
  Returns:
    train_data: Training data (feature + label)
    test_data: Testing data (feature + label)
  """
  
  # Define symmetric covariance matrix for generating features
  cov_matrix = np.random.uniform(0, 1, [dim, dim])
  cov_matrix = 0.5 * (cov_matrix + np.transpose(cov_matrix))
  
  # Generate train/test features
  x_train = np.random.multivariate_normal(np.zeros([dim,]), cov_matrix, [no,])
  x_test = np.random.multivariate_normal(np.zeros([dim,]), cov_matrix, [no,])
  
  # Define feature label relationship
  W = np.random.uniform(0, 1, [dim,])
  b = np.random.uniform(0, 1, 1)
  
  # Generate train/test labels
  y_train = np.matmul(x_train, W) + b + np.random.normal(0, noise_rate, no)
  y_train = np.reshape(1*(y_train), [-1, 1])
  
  y_test = np.matmul(x_test, W) + b + np.random.normal(0, noise_rate, no)
  y_test = np.reshape(1*(y_test), [-1, 1])
    
  train_data = np.concatenate((x_train, y_train), axis=1)
  test_data = np.concatenate((x_test, y_test), axis=1)
  
  # Normalization
  for i in range(dim+1):
    train_data[:, i] = train_data[:, i] - np.min(train_data[:, i])
    train_data[:, i] = train_data[:, i] / (np.max(train_data[:, i]) + 1e-8)
    
  # Normalization
  for i in range(dim+1):
    test_data[:, i] = test_data[:, i] - np.min(test_data[:, i])
    test_data[:, i] = test_data[:, i] / (np.max(test_data[:, i]) + 1e-8)
  
  return train_data, test_data

def pate_lamda (x, teacher_models, lamda):
  '''Returns PATE_lambda(x).
  
  Args:
    - x: feature vector
    - teacher_models: a list of teacher models
    - lamda: parameter
    
  Returns:
    - n0, n1: the number of label 0 and 1, respectively
    - out: label after adding laplace noise.
  '''
      
  y_hat = list()
        
  for teacher in teacher_models:            
    temp_y = teacher.predict(np.reshape(x, [1,-1]))
    y_hat = y_hat + [temp_y]
  
  y_hat = np.asarray(y_hat)
  n0 = sum(y_hat == 0)
  n1 = sum(y_hat == 1)
  
  lap_noise = np.random.laplace(loc=0.0, scale=lamda)
  
  out = (n1+lap_noise) / float(n0+n1)
  out = int(out>0.5)
        
  return n0, n1, out 


def pategan(x_train, parameters):
  '''Basic PATE-GAN framework.
  
  Args:
    - x_train: training data
    - parameters: PATE-GAN parameters
      - n_s: the number of student training iterations
      - batch_size: the number of batch size for training student and generator
      - k: the number of teachers
      - epsilon, delta: Differential privacy parameters
      - lamda: noise size
      
  Returns:
    - x_train_hat: generated training data by differentially private generator
  '''
  
  # Reset the graph
  tf.reset_default_graph()
    
  # PATE-GAN parameters
  # number of student training iterations
  n_s = parameters['n_s']
  # number of batch size for student and generator training
  batch_size = parameters['batch_size']
  # number of teachers
  k = parameters['k']
  # epsilon
  epsilon = parameters['epsilon']
  # delta
  delta = parameters['delta']
  # lamda
  lamda = parameters['lamda']
  
  # Other parameters
  # alpha initialize
  L = 20
  alpha = np.zeros([L])
  # initialize epsilon_hat
  epsilon_hat = 0
    
  # Network parameters
  no, dim = x_train.shape
  # Random sample dimensions
  z_dim = int(dim)
  # Student hidden dimension
  student_h_dim = int(dim)
  # Generator hidden dimension
  generator_h_dim = int(4*dim)  
  
  # Partitioning the data into k subsets
  x_partition = list()
  partition_data_no = int(no/k)
    
  idx = np.random.permutation(no)
    
  for i in range(k):
    temp_idx = idx[int(i*partition_data_no):int((i+1)*partition_data_no)]
    temp_x = x_train[temp_idx, :]      
    x_partition = x_partition + [temp_x]    
  
  # Necessary functions for buidling neural network (NN) models
  # Xavier initialization definition
  def xavier_init(size):
    in_dim = size[0]
    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)
    return tf.random_normal(shape = size, stddev = xavier_stddev)    
        
  # Sample from uniform distribution
  def sample_Z(m, n):
    return np.random.uniform(-1., 1., size = [m, n])
     
  # Placeholder
  # PATE labels
  Y = tf.placeholder(tf.float32, shape = [None, 1])  
  # Random variable    
  Z = tf.placeholder(tf.float32, shape = [None, z_dim])
   
  # NN variables   
  # Student
  S_W1 = tf.Variable(xavier_init([dim, student_h_dim]))
  S_b1 = tf.Variable(tf.zeros(shape=[student_h_dim]))
    
  S_W2 = tf.Variable(xavier_init([student_h_dim,1]))
  S_b2 = tf.Variable(tf.zeros(shape=[1]))

  theta_S = [S_W1, S_W2, S_b1, S_b2]
    
  # Generator
  G_W1 = tf.Variable(xavier_init([z_dim, generator_h_dim]))
  G_b1 = tf.Variable(tf.zeros(shape=[generator_h_dim]))

  G_W2 = tf.Variable(xavier_init([generator_h_dim,generator_h_dim]))
  G_b2 = tf.Variable(tf.zeros(shape=[generator_h_dim]))

  G_W3 = tf.Variable(xavier_init([generator_h_dim,dim]))
  G_b3 = tf.Variable(tf.zeros(shape=[dim]))
    
  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]

  # Models
  def generator(z):
    G_h1 = tf.nn.tanh(tf.matmul(z, G_W1) + G_b1)
    G_h2 = tf.nn.tanh(tf.matmul(G_h1, G_W2) + G_b2)
    G_out = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3)
        
    return G_out
    
  def student(x):
    S_h1 = tf.nn.relu(tf.matmul(x, S_W1) + S_b1)
    S_out = tf.matmul(S_h1, S_W2) + S_b2
        
    return S_out
      
  # Loss  
  G_sample = generator(Z)
  S_fake = student(G_sample)
  
  S_loss = tf.reduce_mean(Y * S_fake) - tf.reduce_mean((1-Y) * S_fake)
  G_loss = -tf.reduce_mean(S_fake)
  
  # Optimizer
  S_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)
              .minimize(-S_loss, var_list=theta_S))
  G_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)
              .minimize(G_loss, var_list=theta_G))
  
  clip_S = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_S]
  
  # Sessions
  sess = tf.Session()
  sess.run(tf.global_variables_initializer())
        
  # Iterations
  while epsilon_hat < epsilon:      
          
    # 1. Train teacher models
    teacher_models = list()
    
    for _ in range(k):
                
      Z_mb = sample_Z(partition_data_no, z_dim)
      G_mb = sess.run(G_sample, feed_dict = {Z: Z_mb})
                
      temp_x = x_partition[i]
      idx = np.random.permutation(len(temp_x[:, 0]))
      X_mb = temp_x[idx[:partition_data_no], :]
                
      X_comb = np.concatenate((X_mb, G_mb), axis = 0)
      Y_comb = np.concatenate((np.ones([partition_data_no,]), 
                               np.zeros([partition_data_no,])), axis = 0)
                
      model = LogisticRegression()
      model.fit(X_comb, Y_comb)
      teacher_models = teacher_models + [model]
            
    # 2. Student training
    for _ in range(n_s):
          
      Z_mb = sample_Z(batch_size, z_dim)
      G_mb = sess.run(G_sample, feed_dict = {Z: Z_mb})
      Y_mb = list()
            
      for j in range(batch_size):                
        n0, n1, r_j = pate_lamda(G_mb[j, :], teacher_models, lamda)
        Y_mb = Y_mb + [r_j]
       
        # Update moments accountant
        q = np.log(2 + lamda * abs(n0 - n1)) - np.log(4.0) - \
            (lamda * abs(n0 - n1))
        q = np.exp(q)
                
        # Compute alpha
        for l in range(L):
          temp1 = 2 * (lamda**2) * (l+1) * (l+2)
          temp2 = (1-q) * ( ((1-q)/(1-q*np.exp(2*lamda)))**(l+1) ) + \
                  q * np.exp(2*lamda * (l+1))
          alpha[l] = alpha[l] + np.min([temp1, np.log(temp2)])
        
      # PATE labels for G_mb  
      Y_mb = np.reshape(np.asarray(Y_mb), [-1,1])
                
      # Update student
      _, D_loss_curr, _ = sess.run([S_solver, S_loss, clip_S], 
                                   feed_dict = {Z: Z_mb, Y: Y_mb})
    
    # Generator Update        
    Z_mb = sample_Z(batch_size, z_dim)
    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict = {Z: Z_mb})
        
    # epsilon_hat computation
    curr_list = list()        
    for l in range(L):
      temp_alpha = (alpha[l] + np.log(1/delta)) / float(l+1)
      curr_list = curr_list + [temp_alpha]
        
    epsilon_hat = np.min(curr_list)    

  # Outputs
  x_train_hat = sess.run([G_sample], feed_dict = {Z: sample_Z(no, z_dim)})[0]
    
  return x_train_hat

def supervised_model_training(x_train, y_train, x_test, 
                              y_test, model_name):
  """Train supervised learning models and report the results.
  
  Args:
    - x_train, y_train: training dataset
    - x_test, y_test: testing dataset
    - model_name: supervised model name such as logisticregression
    
  Returns:
    - auc: Area Under ROC Curve
    - apr: Average Precision and Recall
    - fpr: False Positive Rate
    - tpr: True Positive Rate
    - thres: Threshold value
  """
     
  if model_name == 'logisticregression':
    model  = LogisticRegression()
  elif model_name == 'randomforest':      
    model = RandomForestClassifier()
  elif model_name == 'gaussiannb':  
    model = GaussianNB()
  elif model_name == 'bernoullinb':  
    model        = BernoulliNB()
  elif model_name == 'multinb':  
    model        = MultinomialNB()
  elif model_name == 'svmlin':         
    model        = svm.LinearSVC() 
  elif model_name == 'gbm':         
    model         = GradientBoostingClassifier()   
  elif model_name == 'Extra Trees':
    model =  ExtraTreesClassifier(n_estimators=20)
  elif model_name == 'LDA':
    model =  LinearDiscriminantAnalysis() 
  elif model_name == 'Passive Aggressive':
    model =   PassiveAggressiveClassifier()
  elif model_name == 'AdaBoost':
    model =   AdaBoostClassifier()
  elif model_name == 'Bagging':
    model =   BaggingClassifier()
  elif model_name == 'xgb':
    model =   XGBRegressor()                                
  
  if(model_name=='svmlin' or model_name=='Passive Aggressive'): 
    model.fit(x_train, y_train)
    predict = model.decision_function(x_test)
  elif (model_name =='xgb'):
    model.fit(np.asarray(x_train), y_train)
    predict = model.predict(np.asarray(x_test))
  else:
    model.fit(x_train, y_train)
    predict = model.predict_proba(x_test)[:,1]
        
  predict2= model.predict(x_test)
  accuracy= metrics.accuracy_score(y_test, predict2)
  # print(accuracy)     # use for verification
        
  # AUC / AUPRC Computation
  auc = metrics.roc_auc_score(y_test, predict)
  apr = metrics.average_precision_score(y_test, predict)
  fpr, tpr, thres = metrics.roc_curve(y_test, predict)
  
  return auc,apr,fpr,tpr,thres

scaler = MinMaxScaler()

def pategan_main (dataset, data_no, data_dim, noise_rate, iterations, n_s, batch_size, k, epsilon, delta, lamda):
  """PATE-GAN Main function
  
  Args:
    data_no: number of generated data
    data_dim: number of data dimensions
    noise_rate: noise ratio on data
    iterations: number of iterations for handling initialization randomness
    n_s: the number of student training iterations
    batch_size: the number of batch size for training student and generator
    k: the number of teachers
    epsilon, delta: Differential privacy parameters
    lamda: noise size
    
  Returns:
    - orig_auc: Area Under ROC Curve for Original Dataset
    - orig_apr: Average Precision and Recall for Original Dataset
    - orig_fpr: False Positive Rate for Original Dataset
    - orig_tpr: True Positive Rate for Original Dataset
    - orig_thres: Threshold Value for Original Dataset
    - pate_auc: Area Under ROC Curve for Synthetic Dataset
    - pate_apr: Average Precision and Recall for Synthetic Dataset
    - pate_fpr: False Positive Rate for Synthetic Dataset
    - pate_tpr: True Positive Rate for Synthetic Dataset
    - pate_thres: Threshold Value for Synthetic Dataset
    - train_data: Original Data
    - synth_train_data: Synthetically Generated Data
    """
  
  # Supervised model types
  # Use any of these models
  """
  models = ['logisticregression','randomforest', 'gaussiannb','bernoullinb',
            'svmlin', 'Extra Trees','LDA', 'AdaBoost','Bagging','gbm', 'xgb']"""

  model = 'logisticregression'
  
  # Data generation
  if dataset == 'random':
      train_data, test_data = data_generator(data_no, data_dim, noise_rate)
  elif dataset == 'census':
      # Insert relevant dataset here, and scale between 0 and 1.
      # data = pd.read_csv('census.csv').to_numpy()     # commented for future use
      data = census_df_enc.to_numpy()
      data = scaler.fit_transform(data)
      train_ratio = 0.8
      train = np.random.rand(data.shape[0])<train_ratio 
      train_data, test_data = data[train], data[~train]
      data_dim = data.shape[1]
  
  # Define outputs
  # results = np.zeros([len(models), 10])    # commented for future use
  
  # Define PATEGAN parameters
  parameters = {'n_s': n_s, 'batch_size': batch_size, 'k': k, 
                'epsilon': epsilon, 'delta': delta, 
                'lamda': lamda}
  
  # Generate synthetic training data
  best_perf = 0.0
  
  for it in range(iterations):
    # print('Iteration',it)     # use for debugging
    synth_train_data_temp = pategan(train_data, parameters)
    temp_perf, _, _, _, _ = supervised_model_training(
        synth_train_data_temp[:, :(data_dim-1)], 
        np.round(synth_train_data_temp[:, (data_dim-1)]),
        train_data[:, :(data_dim-1)], 
        np.round(train_data[:, (data_dim-1)]),
        'logisticregression')
    
    # Select best synthetic data
    if temp_perf > best_perf:
      best_perf = temp_perf.copy()
      synth_train_data = synth_train_data_temp.copy()
      
    # print('Iteration: ' + str(it+1))        # use for debugging
    # print('Best-Perf:' + str(best_perf))    # use for verification
  
  # Train supervised models
    
  # Using original data
  orig_auc, orig_apr, orig_fpr, orig_tpr, orig_thres = (
      supervised_model_training(train_data[:, :(data_dim-1)], 
                                np.round(train_data[:, (data_dim-1)]),
                                test_data[:, :(data_dim-1)], 
                                np.round(test_data[:, (data_dim-1)]),
                                model))
      
  # Using synthetic data
  pate_auc, pate_apr, pate_fpr, pate_tpr, pate_thres = (
      supervised_model_training(synth_train_data[:, :(data_dim-1)], 
                                np.round(synth_train_data[:, (data_dim-1)]),
                                test_data[:, :(data_dim-1)], 
                                np.round(test_data[:, (data_dim-1)]),
                                model))

    
  """  
  # Print the results for each iteration
  results = pd.DataFrame(np.round(results, 4), 
                         columns=['AUC-Original', 'AUC-Synthetic', 
                                  'APR-Original', 'APR-Synthetic',
                                  'FPR-Original', 'FPR-Synthetic', 
                                  'TPR-Original', 'TPR-Synthetic',
                                  'Threshold-Original', 'Threshold-Synthetic'])"""
  # Commented - use for verification                            
  # print(orig_auc, orig_apr, orig_fpr, orig_tpr, orig_thres)
  # print(pate_auc, pate_apr, pate_fpr, pate_tpr, pate_thres)  
  # print('Averages:')    
  # print(results.mean(axis=0))
  
  return orig_auc, orig_apr, orig_fpr, orig_tpr, orig_thres, pate_auc, pate_apr, pate_fpr, pate_tpr, pate_thres, train_data, synth_train_data

# run the pate_gan
orig_auc, orig_apr, orig_fpr, orig_tpr, orig_thres, pate_auc, pate_apr, pate_fpr, pate_tpr, pate_thres, ori_data, synth_data = pategan_main(dataset='census', data_no=10000, data_dim=10, noise_rate=1.0, iterations=50, n_s=1, batch_size=64, k=10, epsilon=2.0, delta=0.00001, lamda=1.0)
"""
Variables explained:
lambda (float): PATE noise size
delta (float): Differential privacy parameter
epsilon (float): Differential privacy parameter
k (int): number of teachers
batch_size (int): number of batch size for training student and generator
n_s (int): number of student training iterations
iterations (int): number of iterations for handling initialization randomness
noise_rate (float): noise ratio on data
data_dim (int): number of dimensions of generated dimension (if 'random')
data_no (int): number of generated data
dataset (str): dataset to use ('random', 'credit')
"""

# for plotting the roc curves on original and synthetic datasets
def plot_roc_curve(fpr, tpr):
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

# Print results
print(f"Original dataset       => {orig_auc}, {orig_apr}")
print(f"Synthetic PATE-GAN dataset => {pate_auc}, {pate_apr}")

plot_roc_curve(orig_fpr, orig_tpr)
plot_roc_curve(pate_fpr, pate_tpr)

# post-processing steps to decode the generated data
np.set_printoptions(suppress=False)
original_data = scaler.inverse_transform(ori_data)
synthetic_data = scaler.inverse_transform(synth_data)
np.set_printoptions(suppress=True)

original_data = np.rint(original_data)
original_data = original_data.astype(int)
synthetic_data = np.rint(synthetic_data)
synthetic_data = synthetic_data.astype(int)

# assign columns to the synthetic dataset
syn_df = pd.DataFrame(synthetic_data, columns =['age', 'workclass', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 
                     'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'])

# decoding the values of various categorical columns
dec_df = pd.DataFrame()

dec_df['age'] = syn_df.age

census_df_workclass_enc = enc.fit_transform(census_df.workclass)
census_df_workclass_dec = enc.inverse_transform(syn_df.workclass)
dec_df['workclass'] = census_df_workclass_dec.tolist()

census_df_education_enc = enc.fit_transform(census_df.education)
census_df_education_dec = enc.inverse_transform(syn_df.education)
dec_df['education'] = census_df_education_dec.tolist()

dec_df['education_num'] = syn_df.education_num

census_df_marital_status_enc = enc.fit_transform(census_df.marital_status)
census_df_marital_status_dec = enc.inverse_transform(syn_df.marital_status)
dec_df['marital_status'] = census_df_marital_status_dec.tolist()

census_df_occupation_enc = enc.fit_transform(census_df.occupation)
census_df_occupation_dec = enc.inverse_transform(syn_df.occupation)
dec_df['occupation'] = census_df_occupation_dec.tolist()

census_df_relationship_enc = enc.fit_transform(census_df.relationship)
census_df_relationship_dec = enc.inverse_transform(syn_df.relationship)
dec_df['relationship'] = census_df_relationship_dec .tolist()

census_df_race_enc = enc.fit_transform(census_df.race)
census_df_race_dec = enc.inverse_transform(syn_df.race)
dec_df['race'] = census_df_race_dec.tolist()

census_df_sex_enc = enc.fit_transform(census_df.sex)
census_df_sex_dec = enc.inverse_transform(syn_df.sex)
dec_df['sex'] = census_df_sex_dec.tolist()

dec_df['capital_gain'] = syn_df.capital_gain
dec_df['capital_loss'] = syn_df.capital_loss
dec_df['hours_per_week'] = syn_df.hours_per_week

census_df_native_country_enc = enc.fit_transform(census_df.native_country)
census_df_native_country_dec = enc.inverse_transform(syn_df.native_country)
dec_df['native_country'] = census_df_native_country_dec.tolist()

census_df_income_enc = enc.fit_transform(census_df.income)
census_df_income_dec = enc.inverse_transform(syn_df.income)
dec_df['income'] = census_df_income_dec.tolist()

# run the following code to save the generated dataset as .csv file in Google Drive
"""
dec_df.to_csv('synth_data_pate.csv', sep='\t', encoding='utf-8')

from google.colab import drive
drive.mount('drive')

!cp synth_data_pate.csv "drive/My Drive/"
"""

# upload the .csv file *manually* to GitHub (change the link below accordingly)
# run the following code to retrieve the uploaded .csv file (GAN generated data) and use it for evaluation or further studies
"""
pate_df = pd.read_csv('https://raw.githubusercontent.com/rudrakshkapil09/CMPUT622-Project/main/synth_data_pate.csv', header=None, sep='\t', skiprows=1)

pate_df.columns = pate_df.columns = ['id', 'age', 'workclass', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 
                     'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']

pate_df.drop(['id'], axis=1, inplace=True)
"""